# Домашнее задание по курсу DL-Audio. 
Имплементация модели FastSpeech
Далее предполагается, что все команды запускаются из корневой директории
Для применения модели необходимо:
## Подготовить датасет
1. Установить [датасет LJSpeech,версия 1.1](https://keithito.com/LJ-Speech-Dataset/).
2. Переименовать по необходимости датамет в LJSpeech-1.1 и поместить в директорию `data`.
3. Распаковать в ту же папку `alignments/alignments.zip`
4. Поместить [waveqlow](https://drive.google.com/file/d/1WsibBTsuRg_SF2Z6L6NFRTT-NjEy1oTx/view?usp=sharing) в `model/waveglow/pretrained_model`, переименовать - `waveglow_256channels.pt`;
5. Запустить  `python3 common/preprocess.py`.

## Обучение
Запустить `python3 common/train.py`.

В моем случае скрипт для обучения запускался 3 раза. Для первых двух логи можно найти в папке logger.
Для третьего они не сохранились. Перезапускать для полной версии не стал, так как 40 эпох для понимания скорости
обучения посчитал достаточным. Общее число шагов обучения - 26000, это примерно 70 эпох.

Примечание: для продолжения обучения необходимо при запуске указать параметр --restore_step X,X - число итераций(шагов) обучения. 
В директории model_new должен быть файл checkpoint_X.pth.tar. Он формируется каждые несколько шагов, где несколько - параметр 
save_step в common/hparams.py Был подобран под скорость обучения на колабе.

## Installation Guide:  
В описанном выше hparams.py также указаны все параметры, на которых запускалась модель. Эксперимент
проводился в колабе на GPU, но все переносы на cuda удалены из кода. Для работы в колабе по состоянию на 17-12-2022 
достаточно отдельно установить последнюю строку в requirements.txt   

## Оценивание
Запустить `python3 eval.py`.Внутри этого файла в функции get_data можно указать необходимые для синтеза утверждения
Они будут сохранены в папке results. Там же находится мой результат для трех требуемых к оценке утверждений.



